# LoRA Fine-Tuning of DistilGPT2 on Backtranslated v3 Dataset for Wekeza LLM

This project applies Parameter-Efficient Fine-Tuning (LoRA) to the `distilgpt2` model using a small backtranslated dataset (`v3`) generated via the Self-Alignment with Instruction Backtranslation method. It's part of the ongoing development of **Wekeza LLM**, a local language model assistant focused on Kenya's investment landscape.

## ðŸ§  Objective
To experiment with LoRA fine-tuning on synthetic instructions that simulate real-world financial queries, helping improve the model's ability to generate reliable investment guidance for the Kenyan market.
